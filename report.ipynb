{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 Report\n",
    "\n",
    "Group Members: Greg Mehdiyev, Brendan Artley\n",
    "\n",
    "In this assignment, we were tasked with improving the performance of a neural machine translation task. Given a text file of sentences, we perform the translation with our model and score the results using the BLEU metric. Our group first implemented the correct attention module and then performed unknown word replacement. These two changes improved our overall BLEU score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The first fix was to the attention calculation. This was done by passing the encoder's hidden state through a linear layer and passing the decoder's hidden state through a separate linear layer. The outputs were then added together to get the attention score for each token in the sentence. Then, the softmax function was applied to the attention score vector. This resulted in the correct implementation of the attention module and a BLEU score of 17.11.\n",
    "\n",
    "Fixed Attention\n",
    "```\n",
    "when when i was in my 20s , i had my first <unk> . . \n",
    "i was a and i was particularly in berkeley . berkeley . \n",
    "she was a a woman woman named alex . \n",
    "when the came came up the first first , she came up and she a a a , and she fell into the couch in my office , the her her and and told me , about about about . \n",
    "and when i heard this , i was was . \n",
    "```\n",
    "\n",
    "After implementing the attention module, we noticed that there were still numerous \"UNK\" tokens in our output file. As this token could only hurt our BLEU score, we decided to experiment with replacing this token with a common word. We tested \"a\", \"of\" and \"the\", and found that \"the\" worked best. This improved our BLEU score to 17.15. \n",
    "\n",
    "Unkown Word Replacement\n",
    "```\n",
    "when when i was in my 20s , i had my first the . . \n",
    "i was a and i was particularly in berkeley . berkeley . \n",
    "she was a a woman woman named alex . \n",
    "when the came came up the first first , she came up and she a a a , and she fell into the couch in my office , the her her and and told me , about about about . \n",
    "and when i heard this , i was was . \n",
    "```\n",
    "\n",
    "The next thing that we noticed was that the translated sentence was including the same word in succession. For example, in the first 5 rows of the output.txt file, we can see the \"a\" and \"about\" are repeated 3 times in succession in the same sentence. This is not common in English so we added a condition to remove duplicated words in succession. This increased our BLEU score to 17.25. Finally, we tested the 5 neural machine translation models available in Google Drive. We found that the 'seq2seq_E047.pt' model worked best, and improved our score to 17.49.\n",
    "\n",
    "```\n",
    "when i was in my 20s , i had my first the .\n",
    "i was a and i was particularly in berkeley .\n",
    "she was a woman named alex .\n",
    "as a came in the first meeting , she was sitting and a , and she fell into the office in my office , her and she was me to talk about .\n",
    "and when i heard that , i was .\n",
    "```\n",
    "\n",
    "## Model Iterations\n",
    "\n",
    "Fixed Attention\n",
    "BLEU = 17.11 54.0/24.1/12.0/6.5 (BP = 0.957 ratio = 0.958 hyp_len = 23858 ref_len = 24902)\n",
    "\n",
    "Unknown Word Replacement\n",
    "BLEU = 17.14 54.1/24.1/12.0/6.5 (BP = 0.957 ratio = 0.958 hyp_len = 23858 ref_len = 24902)\n",
    "\n",
    "3x word removals\n",
    "BLEU = 17.15 55.7/24.9/12.5/6.8 (BP = 0.926 ratio = 0.929 hyp_len = 23127 ref_len = 24902)\n",
    "\n",
    "2x word removals\n",
    "BLEU = 17.25 61.8/28.2/14.6/8.1 (BP = 0.809 ratio = 0.825 hyp_len = 20545 ref_len = 24902)\n",
    "\n",
    "047 Model\n",
    "BLEU = 17.49 62.3/28.9/15.1/8.4 (BP = 0.799 ratio = 0.817 hyp_len = 20342 ref_len = 24902)\n",
    "\n",
    "## Contributions\n",
    "\n",
    "Greg and Brendan: Attention Module, Unknown Word Replacement, Word Removals, Testing Different Models, Research. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "6549ab8b689ab7a083d6ad2eb1a3b39e2fe3c4142e49e5864ce987f02abf3471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
